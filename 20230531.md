# 5월 31일

## 투명망토 만들기
- 비디오 설정 후 배경이미지 캡쳐
- 카메라 열고 웹캠으로 읽기
- Color segmentation으로 크로마키할 색을 설정
- 원하는 색을 제거하기
 
```python
# import
import cv2
import numpy as np
import time, argparse

# 비디오
parser = argparse.ArgumentParser()
parser.add_argument('--video', help='Input video path')
args = parser.parse_args()

cap = cv2.VideoCapture(args.video if args.video else 0)
time.sleep(3)

# 1- 배경이미지 캡쳐
for i in range(60):
    ret, background=cap.read()
# 동여상을 저장하기 위한 코드
fourcc = cv2.VideoWriter_fourcc('m','p','4','v')
# 출력 비디오 파일의 세팅
out = cv2.VideoWriter('videos/output.mp4', fourcc, cap.get(cv2.CAP_PROP_FPS), (background.shape[1], background.shape[0]))
out2 = cv2.VideoWriter('videos/output.mp4', fourcc, cap.get(cv2.CAP_PROP_FPS), (background.shape[1], background.shape[0]))

# 2- 카메라 열고 읽기
while(cap.isOpened()):
    # 카메라로 읽어들인 것을 img에 저장
    ret, img = cap.read()
    if not ret:
        break

    # BRG -> HSV
    hsv = cv2.cvtColor(img, cv2.COLOR_BGR2HSV)
    # red를 확인하기 위한 마스킹 작업    
    # color segmentation - mask
    lower_red = np.array([0, 120, 70])
    upper_red = np.array([10, 255, 255])
    mask1 = cv2.inRange(hsv, lower_red, upper_red)

    lower_red = np.array([130, 120, 70])
    upper_red = np.array([180, 255, 255])
    mask2 = cv2.inRange(hsv, lower_red, upper_red)

    mask1 = mask1 + mask2 # 빨간색을 다 mask한 마스크가 탄생
    # 노이즈 제거하기
    
    # Remove noise - 마스크를 색만 뽑으면 노이즈가 발생해서 정제해주는 함수
    mask_cloak = cv2.morphologyEx(mask1, op=cv2.MORPH_OPEN, kernel=np.ones((3,3), np.uint8), iterations=2)
    mask_cloak = cv2.dilate(mask_cloak, kernel=np.ones((3,3), np.uint8), iterations=1)
    mask_bg = cv2.bitwise_not(mask_cloak)

    cv2.imshow('mask_cloak', mask_cloak)
    # 최종 결과물 만들기
    res1 = cv2.bitwise_and(background, background, mask=mask_cloak)  # background에서 mask만 남음
    res2 = cv2.bitwise_and(img, img, mask=mask_bg) # 캠으로 들어오는 이미지에서 mask가 안 된 부분만 남음
    result = cv2.addWeighted(src1=res1, alpha=1, src2=res2, beta=1, gamma=0)

    cv2.imshow('res1', res1)

    cv2.imshow('result', result)
    out.write(result)
    out2.write(img)

    if cv2.waitKey(1) == ord('q'):
        break

out.release()
out2.release()
cap.release()
```
- 붉은색에 대한 HSV 값을 찾아서 코드에 적용시키 웹캠에 붉은색이 잡히면 크로마키처럼 실행
    - 빨간색의 범위는 0~10 170~180이라 두 개로 나누어 마스크 생성 후 더함
-  cv2.inRange() : 범위 안에 해당하는 값들로 마스크를 생성
-  bitwise_and() : AND연산을 통해 두개의 행렬이 0이 아닌 것만 통과되어 마스크 영역만 남음
-  cv2.addWeighted() : 두 개의 이미지를 합친다 
> 실행 결과

- 빨간색을 크로마키한 경우

![Screenshot from 2023-05-31 11-41-08](https://github.com/ajhwan/OpenCV_study/assets/129160008/6183d1e3-0354-4696-a4f4-6cc7f3255ce2)

- 검은색을 크로마키한 경우

![Screenshot from 2023-05-31 11-39-47](https://github.com/ajhwan/OpenCV_study/assets/129160008/3d85a124-f2f3-4b58-a6f8-028f4458be59)

## 이미지 모자이크
- 1. 이미지를 가져오고 사이즈를 조정
- 2. 조각에 맞는 이미지를 미리 확인
- 3. 픽셀의 분포 확인
- 4. 글씨 이미지와 고양이 픽셀 이미지 매칭
- 5. 그림을 채워주기
```python
import cv2, os
import numpy as np
import matplotlib.pyplot as plt
from collections import defaultdict

# 과정
# 1 - 이미지를 가져오고 사이즈를 조정
img_path = 'img/09.jpg'
img = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)
img = cv2.resize(img, dsize=None, fx=0.2, fy=0.2) # 픽셀이 너무 많으면 조각을 찾는 과정이 오래걸려서 최대한 축소시기키 위함
# print(img.shape)
# plt.figure(figsize=(20,20))
# plt.axis('off')
# plt.imshow(img,cmap='gray')
# plt.show()
# 2 - 조각에 맞는 이미지를 미리 확인
sample_imgs = np.load('dataset/k49-train-imgs.npz')['arr_0']
# plt.figure(figsize=(20,10))
# for i in range(80):
#     img_patch = 255 - sample_imgs[i]
#     plt.subplot(5, 16, i+1)
#     plt.title(int(np.mean(img_patch)))
#     plt.axis('off')
#     plt.imshow(img_patch, cmap='gray')
# plt.show()
# 3 - 픽셀의 분포 확인
means = np.mean(255- sample_imgs, axis=(1,2))
# plt.figure(figsize=(12,6))
# plt.hist(means, bins=50, log=True)
# plt.show()

img = cv2.normalize(img, dst=None, alpha=120, beta=245, norm_type=cv2.NORM_MINMAX)
# plt.figure(figsize=(12,6))
# plt.hist(img.flatten(), bins=50, log=True)
# plt.show()
# 4 - 글씨 이미지와 고양이 픽셀이미지 매칭
bins = defaultdict(list)

for img_patch, mean in zip(sample_imgs, means):
    bins[int(mean)].append(img_patch)

print(len(bins))
# 5 - 채우기
h, w = img.shape
img_out = np.zeros((h*28, w*28), dtype=np.uint8)
for y in range(h):
    for x in range(w):
        level = img[y,x]
        b = bins[level]
        while len(b) == 0:
            level +=1
            b = bins[level]
        img_patch = 255 - b[np.random.randint(len(b))]
        img_out[y*28:(y+1)*28, x*28:(x+1)*28] = img_patch
plt.figure(figsize=(20,20))
plt.axis('off')
plt.imshow(img_out, cmap='gray')
plt.show()
```
> 실행 결과
### 이미지를 가져오고 사이즈를 조정
```python
img = cv2.resize(img, dsize=None, fx=0.2, fy=0.2)
print(img.shape)
plt.figure(figsize=(20,20))
plt.axis('off')
plt.imshow(img,cmap='gray')
plt.show()
```
![Screenshot from 2023-05-31 14-44-30](https://github.com/ajhwan/OpenCV_study/assets/129160008/566d1961-22ff-4224-93b8-6cf15b6cfbcc)

### 조각에 맞는 이미지를 미리 확인

```python
sample_imgs = np.load('dataset/k49-train-imgs.npz')['arr_0']
plt.figure(figsize=(20,10))
for i in range(80):
    img_patch = 255 - sample_imgs[i]
    plt.subplot(5, 16, i+1)
    plt.title(int(np.mean(img_patch)))
    plt.axis('off')
    plt.imshow(img_patch, cmap='gray')
plt.show()
```
![Screenshot from 2023-05-31 14-53-43](https://github.com/ajhwan/OpenCV_study/assets/129160008/2612687d-50ac-4132-be7d-2e8c749cbbb0)

### 픽셀의 분포 확인

```python
means = np.mean(255- sample_imgs, axis=(1,2))
plt.figure(figsize=(12,6))
plt.hist(means, bins=50, log=True)
plt.show()

img = cv2.normalize(img, dst=None, alpha=120, beta=245, norm_type=cv2.NORM_MINMAX)
plt.figure(figsize=(12,6))
plt.hist(img.flatten(), bins=50, log=True)
plt.show()
```
![Screenshot from 2023-05-31 14-54-16](https://github.com/ajhwan/OpenCV_study/assets/129160008/b1f5f5ea-a314-46fe-bf5b-bd7d48ad8c7f)

![Screenshot from 2023-05-31 14-59-32](https://github.com/ajhwan/OpenCV_study/assets/129160008/6cbb68c7-e49c-4efb-8dce-ff32ae10bb56)

### 글씨 이미지와 고양이 픽셀 이미지 매칭 및 그림을 채워주기
```python
bins = defaultdict(list)

for img_patch, mean in zip(sample_imgs, means):
    bins[int(mean)].append(img_patch)

print(len(bins))
h, w = img.shape
img_out = np.zeros((h*28, w*28), dtype=np.uint8)
for y in range(h):
    for x in range(w):
        level = img[y,x]
        b = bins[level]
        while len(b) == 0:
            level +=1
            b = bins[level]
        img_patch = 255 - b[np.random.randint(len(b))]
        img_out[y*28:(y+1)*28, x*28:(x+1)*28] = img_patch
plt.figure(figsize=(20,20))
plt.axis('off')
plt.imshow(img_out, cmap='gray')
plt.show()
```
![Screenshot from 2023-05-31 15-10-54](https://github.com/ajhwan/OpenCV_study/assets/129160008/16094aa9-133b-4f3c-a5e5-80b101602e1f)

- img_path, 과 sample_imgs의 데이터를 수정하여 다른 사진의 결과값을 확인 할 수 있다.
- cv2.normalize를 통해서 흑, 백의 분포도를 원하는 범위로 간추려 줄 수 있다.
- defaultdict 에서 patch이미지를 평균값에 맞춰서 bins에 딕셔너릴로 정리한다.
- for문을 반복으로 돌면서 레벨에 해당되는 이미지를 채워 넣는다.
- 이미지들 중에 랜덤으로 뽑아서 img_patch라는 변수에 넣는다.

## 이미지 매칭
- 서로 다른 두 이미지를 비교해서 짝이 맞는 형태의 객체가 있는 찾아내는 기술

- 평균 해시 매치 ( Average Hash Matching )
  - 이미지를 가로 세로 비율과 무관하게 특정한 크기로 축소
  - 픽셀 전체의 평균값을 구해서 각 픽셀의 값이 평균보다 작으면 0, 크면 1로 변환
  - 0 또는 1로만 구성된 각 픽셀 값을 1행 1열로 변환
- 평균 해시를 다른 이미지의 것과 비교해서 얼마나 비슷한 정도를 측정하는 방법으로는 유클리드 거리, 와 해밍 거리가 있음
  - 유클리드 거리는 두 값의 차이로 거리를 계산
  - 해밍 거리는 두 값의 길이가 같아야 계산할 수 있음
       - 해밍 거리는 두 수의 같은 자리 값 중 서로 다른 것이 몇개인지를 판단하여 유사도를 계산

- 템플릿 매칭 ( Template Matching )
    - 템플릿 매칭은 특정 물체에 대한 이미지를 준비해 두고 그 물체가 포함 되어 있을 것이라고 예상할 수 있는 이미지와 비교하여 매칭되는 위치를 찾는 것
    - 미리 준비한 이미지를 템플릿 이미지라고 함
    
    - result = cv2.matchTemplate(img, templ, method, result, mask)
        - img : 입력 이미지
        - templ : 템플릿 이미지
        - method : 매칭 메서드 (cv2.TM_SQDIFF : 제곱 차이 매칭, 완벽 매칭 : 0, 나쁜 매칭 : 큰값    
                             /cv2.TM_SQDIFF_NORMED : 제곱 차이 매칭의 정규화   
                             /cv2.TM_CCORR : 상관관계 매칭, 완벽매칭 : 큰값, 나쁜 매칭 : 0    
                             / cv2.TM_CCORR_NORMED : 상관관계 매칭의 정규화    
                             / cv2.TM_CCOEFF : 상관계수 매칭, 완벽 매칭 : 1, 나쁜 매칭 : -1    
                             / cv2.TM_CCOEFF_NORMED : 상관계수 매칭의 정규화)
        - result(optional) : 매칭 결과, (W - w + 1)x(H - h + 1)크기의 2차원 배열 [여기서 W,H는 입력 이미지의 너비와 높이, w, h는 템플릿 이미지의 너비와 높이]
        - mask(optional) : TM_SQDIFF, TM_CCORR_NORMED인 경우 사용할 마스크
        
    - minVal, maxVal, minLoc, maxLoc = cv2.minMaxLoc(src, mask)
        - src : 입력 1채널 배열
        - minVal, maxVal : 배열 전체에서의 최소 값, 최대값
        - minLoc, maxLoc : 최소 값과 최대 값의 좌표 (x, y)
        
    - cv2.matchTemplate() : 입력 이미지(img)에서 템플릿 이미지(templ)를 슬라이딩하면서 주어진 메서드에 따라 매칭을 수행
    - cv2.minMaxLoc() : 배열의 최솟값 혹은 최댓값을 구하면 원하는 최선의 매칭값과 매칭점을 손쉽게 해주는 함수 
      - 이 함수는 입력 배열에서의 최솟값, 최댓값뿐만 아니라 최솟값, 최댓값의 좌표도 반환함

```python
import cv2

img = cv2.imread('./img/pistol.jpg')
gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)

gray = cv2.resize(gray, (16,16))
avg = gray.mean()
bin = 1 * (gray > avg)
print(bin)
```

> 실행 결과

![Screenshot from 2023-05-31 16-34-50](https://github.com/ajhwan/OpenCV_study/assets/129160008/b0bc1ed5-65f7-4f3f-a22c-7741f773a12a)

- 위의 사진이 아래처럼 변환됨 
  - 밝은 부분을 '1'로 어두운 부분을 '0'으로 표현
 
![Screenshot from 2023-05-31 16-34-14](https://github.com/ajhwan/OpenCV_study/assets/129160008/d58c3cd5-3ec2-4b85-ba48-6f77d613225c)

### 이미지 나타내기
```python
dhash = []
for row in bin.tolist():
    s = ''.join([str(i) for i in row])
    dhash.append('%02x'%(int(s,2)))
dhash = ''.join(dhash)    
print(dhash)

cv2.namedWindow('pistol', cv2.WINDOW_GUI_NORMAL)
cv2.imshow('pistol', img)
cv2.waitKey(0)
```

> 실행 결과

![Screenshot from 2023-05-31 16-42-52](https://github.com/ajhwan/OpenCV_study/assets/129160008/f0aebb2c-5bed-4aa5-831c-18d0cde6a45d)

### 유사한 이미지 찾기
```python
import cv2
import numpy as np
import glob

# 영상 읽기 및 표시
img = cv2.imread('./img/pistol.jpg')
cv2.imshow('query', img)

# 비교할 영상들이 있는 경로
search_dir = './img/101_ObjectCategories'

# 이미지를 16x16 크기의 평균 해쉬로 변환
def img2hash(img):
    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)
    gray = cv2.resize(gray, (16, 16))
    avg = gray.mean()
    bi = 1 * (gray > avg)
    return bi

# 해밍거리 측정 함수
def hamming_distance(a, b):
    a = a.reshape(1,-1)
    b = b.reshape(1,-1)
    # 같은 자리의 값이 서로 다른 것들의 합
    distance = (a !=b).sum()
    return distance

# 권총 영상의 해쉬 구하기 
query_hash = img2hash(img)

# 이미지 데이타 셋 디렉토리의 모든 영상 파일 경로
img_path = glob.glob(search_dir+'/**/*.jpg')
for path in img_path:
    # 데이타 셋 영상 한개 읽어서 표시 
    img = cv2.imread(path)
    cv2.imshow('searching...', img)
    cv2.waitKey(5)
    # 데이타 셋 영상 한개의 해시
    a_hash = img2hash(img)
    # 해밍 거리 산출 ---⑧
    dst = hamming_distance(query_hash, a_hash)
    if dst/256 < 0.25: # 해밍거리 25% 이내만 출력
        print(path, dst/256)
        cv2.imshow(path, img)
cv2.destroyWindow('searching...')
cv2.waitKey(0)
cv2.destroyAllWindows()
```
- 특정 이미지(템플릿 이미지)를 불러와 그 이미지와 비슷한 이미지들을 찾음
  - 해당 코드에서는 권총의 이미지를 가지고 실행 
> 실행 결과

- 불러온 권총 사진

![Screenshot from 2023-05-31 17-08-02](https://github.com/ajhwan/OpenCV_study/assets/129160008/b04157a9-3179-4390-91a8-2daefab17788)

- 많은 사진들 중에서 찾아낸 권총 사진들

![Screenshot from 2023-05-31 17-09-56](https://github.com/ajhwan/OpenCV_study/assets/129160008/1ecd433b-9199-476f-8a80-ad0935d5965d)





