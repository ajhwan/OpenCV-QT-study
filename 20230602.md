# 6월 2일

## 얼굴 검출하기
- dlib : C++로 작성됨 범용 크로스 플랫폼 소프트웨어 라이브러리
- 얼굴을 우선적으로 잡고, 대략적인 위치를 정하고, 대략적인 위치의 좌표를 정해준다.

> - git clone -b test --single-branch https://github.com/AntonSangho/annoying-orange-face.git 로 파일받아오기
> - https://github.com/davisking/dlib-models/blob/master/shape_predictor_68_face_landmarks.dat.bz2 파일 받아오기
> - 사용하고자하는 annoying-orange-face에 bz2파일을 옮긴다.
> - bunzip2 shape_predictor_68_face_landmarks.dat.bz2 로 압축을 풀어준다.

- 과일 이미지 크기조절
- 데이터 셋을 가져오기
- 카메라를 열고, 인식을 해준다.
- 얼굴 인식을 해주고, 눈, 입을 인식한다.
- 인식 데이터를 crop으로 오려주고, 합성을 한다.(cv2.seamlessClone)

```python
import cv2
import numpy as np
import dlib
from imutils import face_utils, resize

# orange_img = cv2.imread('orange.jpg')
orange_img = cv2.imread('apple.jpg')
orange_img = cv2.resize(orange_img, dsize = (512, 512))

detector = dlib.get_frontal_face_detector()
predictor = dlib.shape_predictor('shape_predictor_68_face_landmarks.dat')

# cam version
cap = cv2.VideoCapture(0)
# video version
# cap = cv2.VideoCapture('01.mp4')

while cap.isOpened():
    ret, frame = cap.read()
    if not ret:
        break
    # face selated from frame
    face = detector(frame)
    result = orange_img.copy()

    # face is over ones
    if len(face) > 0:
        face = face[0]
        # face of left, rigth, top, bottom is save
        x1, y1, x2, y2 = face.left(), face.top(), face.right(), face.bottom()
        # just face copyright
        face_img = frame[y1: y2, x1: x2].copy()

        shape = predictor(frame, face)
        shape = face_utils.shape_to_np(shape)

        for p in shape:
            cv2.circle(face_img, center = (p[0]-x1, p[1]-y1), radius=2, color=255, thickness=-1)

        # left eye
        le_x1 = shape[36, 0]
        le_y1 = shape[37, 1]
        le_x2 = shape[39, 0]
        le_y2 = shape[41, 1]

        le_margin = int((le_x2 - le_x1) * 0.18)

        # right eye
        re_x1 = shape[42, 0]
        re_y1 = shape[43, 1]
        re_x2 = shape[45, 0]
        re_y2 = shape[47, 1]

        re_margin = int((re_x2 - re_x1) * 0.18)

        # Crop
        left_eye_img = frame[le_y1-le_margin: le_y2+le_margin, le_x1-le_margin: le_x2+le_margin].copy()
        right_eye_img = frame[re_y1-re_margin: re_y2+re_margin, re_x1-re_margin: re_x2+re_margin].copy()

        left_eye_img = resize(left_eye_img, width=100)
        right_eye_img = resize(right_eye_img, width=100)

        # poison blending
        result = cv2.seamlessClone(left_eye_img, result, np.full(left_eye_img.shape[:2], 255, left_eye_img.dtype), (200, 200), cv2.MIXED_CLONE)
        result = cv2.seamlessClone(right_eye_img, result, np.full(right_eye_img.shape[:2], 255, right_eye_img.dtype), (350, 200), cv2.MIXED_CLONE)


        # mouse
        mouse_x1 = shape[48, 0]
        mouse_y1 = shape[50, 1]
        mouse_x2 = shape[54, 0]
        mouse_y2 = shape[57, 1]

        mouse_margin = int((mouse_x2 - mouse_x1) * 0.1)
        mouse_img = frame[mouse_y1-mouse_margin: mouse_y2+mouse_margin, mouse_x1-mouse_margin: mouse_x2+mouse_margin].copy()
        mouse_img = resize(mouse_img, width=250)
        result = cv2.seamlessClone(mouse_img, result, np.full(mouse_img.shape[:2], 255, mouse_img.dtype), (280, 320), cv2.MIXED_CLONE)

        # cv2.imshow('left eye', left_eye_img)
        # cv2.imshow('right eye', right_eye_img)
        # cv2.imshow('mouse', mouse_img)
        cv2.imshow('result', result)
    
    if cv2.waitKey(1) & 0xFF == ord('q'):
        break
```
- imutils 가 없는경우 pip3 install imutils로 설치를 해준다.

> 실행 결과
- 오렌지로 했을 때

![Screenshot from 2023-06-02 12-06-19](https://github.com/ajhwan/OpenCV_study/assets/129160008/7bf3fcd2-efc7-44f1-a6a2-622a713c457c)

- 사과로 했을 때

![Screenshot from 2023-06-02 12-10-15](https://github.com/ajhwan/OpenCV_study/assets/129160008/f36dc081-a087-4561-8e87-dfa328453fc2)

## 스노우필터 만들기
```python
import cv2, dlib, sys
import numpy as np

scaler = 0.3

# initialize face detector and shape predictor
detector = dlib.get_frontal_face_detector()
predictor = dlib.shape_predictor('shape_predictor_68_face_landmarks.dat')

# 1. load video
cap = cv2.VideoCapture('samples/girl.mp4')
# load overlay image
overlay = cv2.imread('samples/ryan_transparent.png', cv2.IMREAD_UNCHANGED)

# overlay function
def overlay_transparent(background_img, img_to_overlay_t, x, y, overlay_size=None):
  try:
    bg_img = background_img.copy()
    # convert 3 channels to 4 channels
    if bg_img.shape[2] == 3:
      bg_img = cv2.cvtColor(bg_img, cv2.COLOR_BGR2BGRA)

    if overlay_size is not None:
      img_to_overlay_t = cv2.resize(img_to_overlay_t.copy(), overlay_size)

    b, g, r, a = cv2.split(img_to_overlay_t)

    mask = cv2.medianBlur(a, 5)

    h, w, _ = img_to_overlay_t.shape
    roi = bg_img[int(y-h/2):int(y+h/2), int(x-w/2):int(x+w/2)]

    img1_bg = cv2.bitwise_and(roi.copy(), roi.copy(), mask=cv2.bitwise_not(mask))
    img2_fg = cv2.bitwise_and(img_to_overlay_t, img_to_overlay_t, mask=mask)

    bg_img[int(y-h/2):int(y+h/2), int(x-w/2):int(x+w/2)] = cv2.add(img1_bg, img2_fg)

    # convert 4 channels to 4 channels
    bg_img = cv2.cvtColor(bg_img, cv2.COLOR_BGRA2BGR)

    return bg_img
  except Exception:return background_img

face_roi = []
face_sizes = []

# loop
# 2. 비디오를 계속 프레임 단위로 읽어야하므로
while True:
  # read frame buffer from video
  ret, img = cap.read()
  # 프레임이 없으면 종료 
  if not ret:
    break

  # resize frame
  img = cv2.resize(img, (int(img.shape[1] * scaler), int(img.shape[0] * scaler)))
  ori = img.copy()

  # find faces
  if len(face_roi) == 0:
    faces = detector(img, 1)
  else:
    roi_img = img[face_roi[0]:face_roi[1], face_roi[2]:face_roi[3]]
    # cv2.imshow('roi', roi_img)
    faces = detector(roi_img)

  # no faces
  if len(faces) == 0:
    print('no faces!')

  # find facial landmarks
  for face in faces:
    if len(face_roi) == 0:
      dlib_shape = predictor(img, face)
      shape_2d = np.array([[p.x, p.y] for p in dlib_shape.parts()])
    else:
      dlib_shape = predictor(roi_img, face)
      shape_2d = np.array([[p.x + face_roi[2], p.y + face_roi[0]] for p in dlib_shape.parts()])

    for s in shape_2d:
      cv2.circle(img, center=tuple(s), radius=1, color=(255, 255, 255), thickness=2, lineType=cv2.LINE_AA)

    # compute face center
    #center_x, center_y = np.mean(shape_2d, axis=0).astype(np.int)
    center_x, center_y = np.mean(shape_2d, axis=0).astype(np.int32)

    # compute face boundaries
    min_coords = np.min(shape_2d, axis=0)
    max_coords = np.max(shape_2d, axis=0)

    # draw min, max coords
    cv2.circle(img, center=tuple(min_coords), radius=1, color=(255, 0, 0), thickness=2, lineType=cv2.LINE_AA)
    cv2.circle(img, center=tuple(max_coords), radius=1, color=(255, 0, 0), thickness=2, lineType=cv2.LINE_AA)

    # compute face size
    face_size = max(max_coords - min_coords)
    face_sizes.append(face_size)
    if len(face_sizes) > 10:
      del face_sizes[0]
    mean_face_size = int(np.mean(face_sizes) * 1.8)

    # compute face roi
    face_roi = np.array([int(min_coords[1] - face_size / 2), int(max_coords[1] + face_size / 2), int(min_coords[0] - face_size / 2), int(max_coords[0] + face_size / 2)])
    face_roi = np.clip(face_roi, 0, 10000)

    # draw overlay on face
    result = overlay_transparent(ori, overlay, center_x + 8, center_y - 25, overlay_size=(mean_face_size, mean_face_size))

  # visualize
  cv2.imshow('original', ori)
  cv2.imshow('facial landmarks', img)
  cv2.imshow('result', result)
  if cv2.waitKey(1) == ord('q'):
    sys.exit(1)
```

> 실행 결과
- mp4 파일로 실행

![Screenshot from 2023-06-02 12-45-08](https://github.com/ajhwan/OpenCV_study/assets/129160008/5c4e4fa9-56e1-4587-bbd5-041bb4f1320a)

- 웹캠으로 실행

![Screenshot from 2023-06-02 13-01-36](https://github.com/ajhwan/OpenCV_study/assets/129160008/b3830061-e73b-41c0-a2cc-439a2ebb21a8)


## 강아지 얼굴인식
- dogHeadDetector.dat
- landmarkDetector.dat
- 위의 두 파일을 사용하여 실행

```python
import matplotlib.pyplot as plt
import cv2, dlib, os
import numpy as np
from imutils import face_utils

detector = dlib.cnn_face_detection_model_v1('dogHeadDetector.dat')
predictor = dlib.shape_predictor('landmarkDetector.dat')
# 이미지 불러오기
img_path = 'img/18.jpg'
filename, ext = os.path.splitext(os.path.basename(img_path))
img = cv2.imread(img_path)
img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
# 얼굴인식
dets = detector(img, upsample_num_times=1)
print(dets)
img_result = img.copy()

for i,d in enumerate(dets):
    print("Detection {}: Left: {} Top: {} Right: {} Bottom: {} Confidence: {}" 
          .format(i, d.rect.left(), d.rect.top(), d.rect.right(), d.rect.bottom(), d.confidence))
    x1, y1 = d.rect.left(), d.rect.top()
    x2, y2 = d.rect.right(), d.rect.bottom()
    cv2.rectangle(img_result, pt1=(x1, y1), pt2=(x2,y2), thickness=2, color=(255,0,0), lineType=cv2.LINE_AA)
```
- 강아지의 얼굴을 인식하고 예측하기 위해 두 개의 (.dat)파일을 사용
- 불러온 사진에서 강아지 얼굴이 인식되면 rectangle 함수를 사용하여 강아지 얼굴에 사각형을 만들어 줌  

> 실행 결과

- 강아지 얼굴을 찾아 사각형을 해줌(뒷쪽에 있는 강아지는 어두워서 인식이 잘 안됨)

![Screenshot from 2023-06-02 15-01-42](https://github.com/ajhwan/OpenCV_study/assets/129160008/f77c5ac3-38bd-412d-9846-ce4809db3d9a)

- 예측값에 대한 수치

![Screenshot from 2023-06-02 15-00-55](https://github.com/ajhwan/OpenCV_study/assets/129160008/6210d1ef-137c-4cc5-8946-915e4e9e5b51)

```python
#눈, 코, 입을 찾기(랜드마크)
shapes=[]

for i,d in enumerate(dets):
    shape = predictor(img, d.rect)
    shape = face_utils.shape_to_np(shape)

    for i, p in enumerate(shape):
        shapes.append(shape)
        cv2.circle(img_result,center=tuple(p), radius=3, color=(0,0,255), thickness=-1, lineType=cv2.LINE_AA)
        cv2.putText(img_result, str(i), tuple(p), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255,255,255), 1, cv2.LINE_AA)

img_out = cv2.cvtColor(img_result, cv2.COLOR_RGB2BGR)
cv2.imwrite('img/%s_out%s' % (filename, ext), img_out)
plt.figure(figsize=(16,16))
plt.imshow(img_result)
plt.show()
```
- circle 함수를 통해 눈, 코, 입을 찾아서 점으로 표시
- putText 함수를 통해 점의 위치에 따라 숫자를 입력해서 표시
> 실행 결과

![Screenshot from 2023-06-02 15-35-07](https://github.com/ajhwan/OpenCV_study/assets/129160008/34c0b664-1dc8-4870-993d-0b4907b4b33d)



